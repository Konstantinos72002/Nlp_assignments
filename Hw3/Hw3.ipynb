{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66593,"databundleVersionId":7351814,"sourceType":"competition"},{"sourceId":7575642,"sourceType":"datasetVersion","datasetId":4410752}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch torchvision gensim","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -m spacy download el_core_news_sm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom unidecode import unidecode\nimport re\nimport spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem import PorterStemmer\nspacy.load('el_core_news_sm')\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nnlp = spacy.load(\"el_core_news_sm\")\n# Imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport os\nimport numpy as np\n\nimport torchvision\nimport torchvision.datasets as dsets\nimport torchvision.transforms as transforms\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport sklearn.metrics\nimport seaborn as sns\nimport random\nimport sys\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import data","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\ndf = pd.read_csv(\"/kaggle/input/ys19-2023-assignment-3/train_set.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/ys19-2023-assignment-3/test_set.csv\")\nvalid_df = pd.read_csv(\"/kaggle/input/ys19-2023-assignment-3/valid_set.csv\")\n\ndf['Label_Map'] = df['Sentiment'].map({\n    'NEGATIVE' : 0, 'NEUTRAL' : 1, 'POSITIVE' : 2\n})\nvalid_df['Label_Map'] = valid_df['Sentiment'].map({\n    'NEGATIVE' : 0, 'NEUTRAL' : 1, 'POSITIVE' : 2\n})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing functions","metadata":{}},{"cell_type":"code","source":"def lowercase_without_accentds(text):\n    chars = \"ΆΈΊΌΎΉάέίόύήώ\"\n    lchars = \"αειουηαειουηω\"\n    return text.translate(str.maketrans(chars,lchars))\n\ndef extract_greek_characters(input_string):\n    greek_pattern = re.compile('[Α-Ωα-ω]+')\n    greek_characters = greek_pattern.findall(input_string)\n    result = ' '.join(greek_characters)\n    return result\n\ngreek_stopwords = [\"ο\",\"ειναι\",\"απο\",\"https\",\"co\",\"λεπω\",\"λεμε\",\"ειπα\",\"ειπες\",\"εβαλα\",\"αδιακοπα\",\"αι\",\"ακομα\",\"ακομη\",\"ακριβως\",\"αληθεια\",\"αληθινα\",\"αλλα\",\"αλλαχου\",\"αλλες\",\"αλλη\",\"αλλην\",\"αλλης\",\"αλλιως\",\"αλλιωτικα\",\"αλλο\",\"αλλοι\",\"αλλοιως\",\"αλλοιωτικα\",\"αλλον\",\"αλλος\",\"αλλοτε\",\"αλλου\",\"αλλους\",\"αλλων\",\"αμα\",\"αμεσα\",\"αμεσως\",\"αν\",\"ανα\",\"αναμεσα\",\"αναμεταξυ\",\"ανευ\",\"αντι\",\"αντιπερα\",\"αντις\",\"ανω\",\"ανωτερω\",\"αξαφνα\",\"απ\",\"απεναντι\",\"απο\",\"αποψε\",\"αρα\",\"αραγε\",\"αργα\",\"αργοτερο\",\"αριστερα\",\"αρκετα\",\"αρχικα\",\"ας\",\"αυριο\",\"αυτα\",\"αυτες\",\"αυτη\",\"αυτην\",\"αυτης\",\"αυτο\",\"αυτοι\",\"αυτον\",\"αυτος\",\"αυτου\",\"αυτους\",\"αυτων\",\"αφοτου\",\"αφου\",\"βεβαια\",\"βεβαιοτατα\",\"γι\",\"για\",\"γρηγορα\",\"γυρω\",\"δα\",\"δε\",\"δεινα\",\"δεν\",\"δεξια\",\"δηθεν\",\"δηλαδη\",\"δι\",\"δια\",\"διαρκως\",\"δικα\",\"δικο\",\"δικοι\",\"δικος\",\"δικου\",\"δικους\",\"διολου\",\"διπλα\",\"διχως\",\"εαν\",\"εαυτο\",\"εαυτον\",\"εαυτου\",\"εαυτους\",\"εαυτων\",\"εγκαιρα\",\"εγκαιρως\",\"εγω\",\"εδω\",\"ειδεμη\",\"ειθε\",\"ειμαι\",\"ειμαστε\",\"ειναι\",\"εις\",\"εισαι\",\"εισαστε\",\"ειστε\",\"ειτε\",\"ειχα\",\"ειχαμε\",\"ειχαν\",\"ειχατε\",\"ειχε\",\"ειχες\",\"εκαστα\",\"εκαστες\",\"εκαστη\",\"εκαστην\",\"εκαστης\",\"εκαστο\",\"εκαστοι\",\"εκαστον\",\"εκαστος\",\"εκαστου\",\"εκαστους\",\"εκαστων\",\"εκει\",\"εκεινα\",\"εκεινες\",\"εκεινη\",\"εκεινην\",\"εκεινης\",\"εκεινο\",\"εκεινοι\",\"εκεινον\",\"εκεινος\",\"εκεινου\",\"εκεινους\",\"εκεινων\",\"εκτος\",\"εμας\",\"εμεις\",\"εμενα\",\"εμπρος\",\"εν\",\"ενα\",\"εναν\",\"ενας\",\"ενος\",\"εντελως\",\"εντος\",\"εντωμεταξυ\",\"ενω\",\"εξ\",\"εξαφνα\",\"εξης\",\"εξισου\",\"εξω\",\"επανω\",\"επειδη\",\"επειτα\",\"επι\",\"επισης\",\"επομενως\",\"εσας\",\"εσεις\",\"εσενα\",\"εστω\",\"εσυ\",\"ετερα\",\"ετεραι\",\"ετερας\",\"ετερες\",\"ετερη\",\"ετερης\",\"ετερο\",\"ετεροι\",\"ετερον\",\"ετερος\",\"ετερου\",\"ετερους\",\"ετερων\",\"ετουτα\",\"ετουτες\",\"ετουτη\",\"ετουτην\",\"ετουτης\",\"ετουτο\",\"ετουτοι\",\"ετουτον\",\"ετουτος\",\"ετουτου\",\"ετουτους\",\"ετουτων\",\"ετσι\",\"ευγε\",\"ευθυς\",\"ευτυχως\",\"εφεξης\",\"εχει\",\"εχεις\",\"εχετε\",\"εχθες\",\"εχομε\",\"εχουμε\",\"εχουν\",\"εχτες\",\"εχω\",\"εως\",\"η\",\"ηδη\",\"ημασταν\",\"ημαστε\",\"ημουν\",\"ησασταν\",\"ησαστε\",\"ησουν\",\"ηταν\",\"ητανε\",\"ητοι\",\"ηττον\",\"θα\",\"ι\",\"ιδια\",\"ιδιαν\",\"ιδιας\",\"ιδιες\",\"ιδιο\",\"ιδιοι\",\"ιδιον\",\"ιδιος\",\"ιδιου\",\"ιδιους\",\"ιδιων\",\"ιδιως\",\"ιι\",\"ιιι\",\"ισαμε\",\"ισια\",\"ισως\",\"καθε\",\"καθεμια\",\"καθεμιας\",\"καθενα\",\"καθενας\",\"καθενος\",\"καθετι\",\"καθολου\",\"καθως\",\"και\",\"κακα\",\"κακως\",\"καλα\",\"καλως\",\"καμια\",\"καμιαν\",\"καμιας\",\"καμποσα\",\"καμποσες\",\"καμποση\",\"καμποσην\",\"καμποσης\",\"καμποσο\",\"καμποσοι\",\"καμποσον\",\"καμποσος\",\"καμποσου\",\"καμποσους\",\"καμποσων\",\"κανεις\",\"κανεν\",\"κανενα\",\"κανεναν\",\"κανενας\",\"κανενος\",\"καποια\",\"καποιαν\",\"καποιας\",\"καποιες\",\"καποιο\",\"καποιοι\",\"καποιον\",\"καποιος\",\"καποιου\",\"καποιους\",\"καποιων\",\"καποτε\",\"καπου\",\"καπως\",\"κατ\",\"κατα\",\"κατι\",\"κατιτι\",\"κατοπιν\",\"κατω\",\"κιολας\",\"κλπ\",\"κοντα\",\"κτλ\",\"κυριως\",\"λιγακι\",\"λιγο\",\"λιγωτερο\",\"λογω\",\"λοιπα\",\"λοιπον\",\"μα\",\"μαζι\",\"μακαρι\",\"μακρυα\",\"μαλιστα\",\"μαλλον\",\"μας\",\"με\",\"μεθαυριο\",\"μειον\",\"μελει\",\"μελλεται\",\"μεμιας\",\"μεν\",\"μερικα\",\"μερικες\",\"μερικοι\",\"μερικους\",\"μερικων\",\"μεσα\",\"μετ\",\"μετα\",\"μεταξυ\",\"μεχρι\",\"μη\",\"μηδε\",\"μην\",\"μηπως\",\"μητε\",\"μια\",\"μιαν\",\"μιας\",\"μολις\",\"μολονοτι\",\"μοναχα\",\"μονες\",\"μονη\",\"μονην\",\"μονης\",\"μονο\",\"μονοι\",\"μονομιας\",\"μονος\",\"μονου\",\"μονους\",\"μονων\",\"μου\",\"μπορει\",\"μπορουν\",\"μπραβο\",\"μπρος\",\"να\",\"ναι\",\"νωρις\",\"ξανα\",\"ξαφνικα\",\"ο\",\"οι\",\"ολα\",\"ολες\",\"ολη\",\"ολην\",\"ολης\",\"ολο\",\"ολογυρα\",\"ολοι\",\"ολον\",\"ολονεν\",\"ολος\",\"ολοτελα\",\"ολου\",\"ολους\",\"ολων\",\"ολως\",\"ολωςδιολου\",\"ομως\",\"οποια\",\"οποιαδηποτε\",\"οποιαν\",\"οποιανδηποτε\",\"οποιας\",\"οποιαςδηποτε\",\"οποιδηποτε\",\"οποιες\",\"οποιεςδηποτε\",\"οποιο\",\"οποιοδηποτε\",\"οποιοι\",\"οποιον\",\"οποιονδηποτε\",\"οποιος\",\"οποιοςδηποτε\",\"οποιου\",\"οποιουδηποτε\",\"οποιους\",\"οποιουςδηποτε\",\"οποιων\",\"οποιωνδηποτε\",\"οποτε\",\"οποτεδηποτε\",\"οπου\",\"οπουδηποτε\",\"οπως\",\"ορισμενα\",\"ορισμενες\",\"ορισμενων\",\"ορισμενως\",\"οσα\",\"οσαδηποτε\",\"οσες\",\"οσεςδηποτε\",\"οση\",\"οσηδηποτε\",\"οσην\",\"οσηνδηποτε\",\"οσης\",\"οσηςδηποτε\",\"οσο\",\"οσοδηποτε\",\"οσοι\",\"οσοιδηποτε\",\"οσον\",\"οσονδηποτε\",\"οσος\",\"οσοςδηποτε\",\"οσου\",\"οσουδηποτε\",\"οσους\",\"οσουςδηποτε\",\"οσων\",\"οσωνδηποτε\",\"οταν\",\"οτι\",\"οτιδηποτε\",\"οτου\",\"ου\",\"ουδε\",\"ουτε\",\"οχι\",\"παλι\",\"παντοτε\",\"παντου\",\"παντως\",\"παρα\",\"περα\",\"περι\",\"περιπου\",\"περισσοτερο\",\"περσι\",\"περυσι\",\"πια\",\"πιθανον\",\"πιο\",\"πισω\",\"πλαι\",\"πλεον\",\"πλην\",\"ποια\",\"ποιαν\",\"ποιας\",\"ποιες\",\"ποιο\",\"ποιοι\",\"ποιον\",\"ποιος\",\"ποιου\",\"ποιους\",\"ποιων\",\"πολυ\",\"ποσες\",\"ποση\",\"ποσην\",\"ποσης\",\"ποσοι\",\"ποσος\",\"ποσους\",\"ποτε\",\"που\",\"πουθε\",\"πουθενα\",\"πρεπει\",\"πριν\",\"προ\",\"προκειμενου\",\"προκειται\",\"προπερσι\",\"προς\",\"προτου\",\"προχθες\",\"προχτες\",\"πρωτυτερα\",\"πως\",\"σαν\",\"σας\",\"σε\",\"σεις\",\"σημερα\",\"σιγα\",\"σου\",\"στα\",\"στη\",\"στην\",\"στης\",\"στις\",\"στο\",\"στον\",\"στου\",\"στους\",\"στων\",\"συγχρονως\",\"συν\",\"συναμα\",\"συνεπως\",\"συνηθως\",\"συχνα\",\"συχνας\",\"συχνες\",\"συχνη\",\"συχνην\",\"συχνης\",\"συχνο\",\"συχνοι\",\"συχνον\",\"συχνος\",\"συχνου\",\"συχνου\",\"συχνους\",\"συχνων\",\"συχνως\",\"σχεδον\",\"σωστα\",\"τα\",\"ταδε\",\"ταυτα\",\"ταυτες\",\"ταυτη\",\"ταυτην\",\"ταυτης\",\"ταυτο,ταυτον\",\"ταυτος\",\"ταυτου\",\"ταυτων\",\"ταχα\",\"ταχατε\",\"τελικα\",\"τελικως\",\"τες\",\"τετοια\",\"τετοιαν\",\"τετοιας\",\"τετοιες\",\"τετοιο\",\"τετοιοι\",\"τετοιον\",\"τετοιος\",\"τετοιου\",\"τετοιους\",\"τετοιων\",\"τη\",\"την\",\"της\",\"τι\",\"τιποτα\",\"τιποτε\",\"τις\",\"το\",\"τοι\",\"τον\",\"τος\",\"τοσα\",\"τοσες\",\"τοση\",\"τοσην\",\"τοσης\",\"τοσο\",\"τοσοι\",\"τοσον\",\"τοσος\",\"τοσου\",\"τοσους\",\"τοσων\",\"τοτε\",\"του\",\"τουλαχιστο\",\"τουλαχιστον\",\"τους\",\"τουτα\",\"τουτες\",\"τουτη\",\"τουτην\",\"τουτης\",\"τουτο\",\"τουτοι\",\"τουτοις\",\"τουτον\",\"τουτος\",\"τουτου\",\"τουτους\",\"τουτων\",\"τυχον\",\"των\",\"τωρα\",\"υπ\",\"υπερ\",\"υπο\",\"υποψη\",\"υποψιν\",\"υστερα\",\"φετος\",\"χαμηλα\",\"χθες\",\"χτες\",\"χωρις\",\"χωριστα\",\"ψηλα\",\"ω\",\"ωραια\",\"ως\",\"ωσαν\",\"ωσοτου\",\"ωσπου\",\"ωστε\",\"ωστοσο\",\"ωχ\",\"ο\",\"η\",\"το\",\"τα\",\"τη\",\"δηλαδη\",\"μεχρι\",\"γιατι\",\"εχω\",\"στους\",\"μια\",\"ένας\",\"μία\",\"κάποιος\",\"κάποια\",\"κάποιο\",\"κάποιοι\",\"αυτος\",\"αυτη\",\"αυτο\",\"αυτοι\",\"αυτες\",\"αυτα\",\"στο\",\"στη\",\"στα\",\"για\",\"με\",\"απο\",\"προς\",\"ειναι\",\"εχει\",\"εχουν\",\"θα\",\"δεν\",\"πανω\",\"κατω\",\"μεσα\",\"εξω\",\"κατω\",\"ως\",\"πανω\",\"κατω\",\"πιο\",\"εδω\",\"εκει\",\"πολυ\",\"λιγο\",\"τωρα\",\"ακομα\",\"ομως\",\"επισης\",\"παντα\",\"ακομη\",\"πιθανως\",\"μονο\",\"οχι\",\"ναι\",\"ευχαριστως\",\"γενικα\",\"ολοι\",\"ολες\",\"ολα\",\"ποιος\",\"ποια\",\"ποιο\",\"ποιοι\",\"ποιες\",\"τιποτα\",\"κανεις\",\"καμια\",\"κανενα\",\"κανενες\",\"αυτος\",\"αυτη\",\"αυτο\",\"αυτοι\",\"αυτες\",\"αυτα\",\"απο\",\"σε\",\"υπο\",\"μετα\",\"πριν\",\"επειτα\",\"αντι\",\"εναν\",\"μιαν\",\"κανεναν\",\"καμιαν\",\"κανενα\",\"καμια\",\"μιαν\",\"ενα\",\"οποιος\",\"οποια\",\"οποιο\",\"οποιοι\",\"οποιες\",\"οποιαν\",\"οποιον\",\"ολος\",\"ολη\",\"ολα\",\"ολους\",\"ολες\",\"ολων\",\"καθενας\",\"καθεμια\",\"καθενα\",\"καθενες\",\"ακομα\",\"ενω\",\"επομενως\",\"συνεπως\",\"επιπλεον\",\"παρολα αυτα\",\"παρ ολα αυτα\",\"επισης\",\"και\",\"αλλα\",\"αλλα και\",\"αν\",\"εαν\",\"αν και\",\"αντι\",\"αντι να\",\"αντι το\",\"αντι τα\",\"αντι του\",\"αντι τη\",\"αντι των\",\"αντι στο\",\"αντι στη\",\"αντι στα\",\"αντι στου\",\"αντι στην\",\"αντι στις\",\"αντι στον\",\"μεσω\",\"τους\",\"μας\",\"ηταν\",\"εκ\",\"φορα\",\"πρωτη\",\"ειχα\",\"εμεις\",\"εσεις\",\"ηδη\",\"απ\",\"εγινε\",\"ειχε\",\"αλλα\",\"ουτε\",\"ενας\",\"εσας\",\"αυτοι\",\"αυτο\",\"νεα\",\"οντως\",\"θελετε\",\"κανει\",\"σ\",\"μας\",\"πρεπε\",\"ε\",\"μαλιστα\",\"τους\",\"ηθελε\",\"παω\",\"εβαλε\",\"λεει\",\"γ\",\"ν\",\"θες\",\"ερχεται\",\"διαρκεια\",\"θελουν\",\"ασε\",\"χ\",\"λες\",\"ξερω\",\"α\",\"δω\",\"ειδε\",\"μπηκε\",\"βαλει\",\"μερες\",\"εφοσον\",\"ενα\",\"δυο\",\"τρια\",\"γινει\",\"εργο\",\"μιλαω\",\"μιλησε\",\"ποσα\",\"ωρες\",\"πρωινες\",\"πρωτα\",\"θελει\",\"βαζω\",\"εβαζε\",\"εναντι\",\"μπορεις\",\"βρισκει\",\"δει\",\"μπορω\",\"γινε\",\"κανουν\"]\npattern = '#!$%^&*()_+={}\\[\\]:;\"\\'<>,.?/\\\\|`~-><]'\n\ndef process_text(text):\n    \n#     new_text = extract_greek_characters(text)\n    doc = nlp(text)\n    processed_words = []\n    # Loop through each word\n    for word in doc:\n        if word.is_punct or any(char.isdigit() for char in word.text) or word.text in greek_stopwords:\n            continue\n        else:\n            clean_word = word.lemma_\n            clean_word = lowercase_without_accentds(clean_word)\n            clean_word = clean_word.lower()\n            if clean_word not in greek_stopwords:\n                processed_words.append(clean_word)\n    # Join the processed words back into a string\n    processed_text = ' '.join(processed_words)\n    return processed_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preprocessing","metadata":{}},{"cell_type":"code","source":"import os\n# os.remove('train.json')\n# os.remove('valid.json')\ntext = df['Text']\nvalid_text = valid_df['Text']\ntest_text = test_df['Text']\nprint(\"Preproccesing training data\")\ndf['processed_text'] = text.apply(process_text)\ndf.to_json('train.json', orient='records', lines=True)\nprint(\"Preproccesing valid data\")\nvalid_df['processed_text'] = valid_text.apply(process_text)\nvalid_df.to_json('valid.json', orient='records', lines=True)\ntest_df['processed_text'] = test_text.apply(process_text)\ntest_df.to_json('test.json', orient='records', lines=True)\nprint(\"Vectorization\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get preprocessed data from file","metadata":{}},{"cell_type":"code","source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Write the DataFrame to a JSON file\ndf_read = pd.read_json('/kaggle/input/processed-dataset/train.json', lines=True)\ntrain_preprocessed_text = df_read['processed_text']\nvalid_df_read = pd.read_json('/kaggle/input/processed-dataset/valid.json', lines=True)\nvalid_preprocessed_text = valid_df_read['processed_text']\ntest_df_read = pd.read_json('/kaggle/input/processed-dataset/test.json', lines=True)\ntest_preprocessed_text = test_df_read['processed_text']\n\nprint(\"TRAIN\")\nprint(len(train_preprocessed_text))\nprint(\"VALID\")\nprint(len(valid_preprocessed_text))\nprint(\"TEST\")\nprint(len(test_preprocessed_text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data tokenization","metadata":{}},{"cell_type":"code","source":"import re\nfrom nltk.tokenize import word_tokenize\n\ndef clean_and_tokenize(text):\n\n    pattern = r'[!@#$%^&*()_+={}\\[\\]:;<>,.?/~`]+'\n    \n    cleaned_text = re.sub(pattern, '', text)\n\n    tokens = cleaned_text.split()\n    \n    return tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize model","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\nimport nltk\nnltk.download('punkt')\n\n# Preprocess each sentence\ndf_read['Tokenized_text'] = train_preprocessed_text.apply(clean_and_tokenize)\nvalid_df_read['Tokenized_text'] = valid_preprocessed_text.apply(clean_and_tokenize)\ntest_df_read['Tokenized_text'] = test_preprocessed_text.apply(clean_and_tokenize)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize model","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom gensim.models import Word2Vec\nimport pandas as pd\nimport numpy as np\n\n# Assuming df_read and valid_df_read are your dataframes with 'Tokenized_text' column\n\n# Combine the tokenized text from both dataframes\nall_tokenized_text = pd.concat([df_read['Tokenized_text'], valid_df_read['Tokenized_text'],test_df_read['Tokenized_text']])\n\n# Define the vector_size (you need to set it to the appropriate value)\nvector_size = 100\n\n# Visualized words\nvisualized_words = ['μητσοτακης', 'τσιπρας', 'νδ', 'συριζα', 'κουλη', 'κκε', 'πολιτης', 'τσιπρα', 'βουλευτης', 'κυριακος',\n                   'προθυπουργος' , 'πασοκ' , 'πολιτικος' , 'δημοκρατια' , 'νεος' , 'βελοπουλος']\n\ndef visualize_embeddings(embeddings, words, axs, title):\n    tsne = TSNE(init = \"pca\",n_components=2,random_state=42, perplexity=len(words) - 1)\n    embedding_vectors = np.array([embeddings[word] for word in words])\n    two_d_embeddings = tsne.fit_transform(embedding_vectors)\n\n    for i, word in enumerate(words):\n        x, y = two_d_embeddings[i, :]\n        axs.scatter(x, y)\n        axs.annotate(word, (x, y), xytext=(5, 2), textcoords=\"offset points\", ha=\"right\", va=\"bottom\")\n\n    axs.set_title(title)\n\n# Create subplots\nfig, axs = plt.subplots(2, 3, figsize=(15, 10))\n\n# # Iterate over the subplots\ncount = 2\nfor i in range(2):\n    for j in range(3):\n        axs_ = axs[i, j]\n\n        # Train Word2Vec model with CBOW algorithm (sg=0)\n        w2v_model = Word2Vec(sentences=all_tokenized_text, vector_size=vector_size, window = count, min_count=1, sg=0)\n        \n        # Visualize embeddings\n        visualize_embeddings(w2v_model.wv, visualized_words, axs_, f'CBOW, Window Size: {count}')\n        count += 3\n# Show the plots\nplt.show()\n\n# Create new subplots for the second set of visualizations\nfig, axs = plt.subplots(2, 3, figsize=(15, 10))\ncount = 2\nfor i in range(2):\n    for j in range(3):\n        axs_ = axs[i, j]\n\n        # Train Word2Vec model with Skip-gram algorithm (sg=1)\n        w2v_model = Word2Vec(sentences=all_tokenized_text, vector_size=vector_size, window=count, min_count=1, sg=1)\n\n        # Visualize embeddings\n        visualize_embeddings(w2v_model.wv, visualized_words, axs_, f'Skip-gram, Window Size: {count}')\n        count += 3\n\n# Show the plots\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train word2vec","metadata":{}},{"cell_type":"code","source":"vector_size = 100\nw2v_model = Word2Vec(sentences=pd.concat([df_read['Tokenized_text'], valid_df_read['Tokenized_text'],test_df_read['Tokenized_text']]), vector_size=vector_size, window=8, min_count=1, sg=0)\nw2v_model.save(\"greek_word2vec.model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create training torches","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.nn.utils.rnn import pad_sequence\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom statistics import mean\n\n# Load the Word2Vec model\nw2v_model = Word2Vec.load(\"greek_word2vec.model\")\n\n# Function to get word embeddings for a tokenized tweet\ndef get_word_embeddings(tokenized_tweet, model):\n    embeddings = []\n    if not tokenized_tweet:\n        tokenized_tweet = [\"Ολυμπιακός\"] # word without sence\n    for word in tokenized_tweet:        \n        if word in model.wv:\n            embeddings.append(model.wv[word])\n        else:\n            embeddings.append(np.zeros(model.vector_size, dtype=np.float32))\n    return embeddings\n\n# Function to get mean embeddings for a list of tweets\ndef get_embeddings_by_mean(tweets):\n    return np.mean(np.stack(tweets, axis=0), axis=0)\n\ndef padding_and_torching(l):\n    max_size = 0\n    list_of_lengths = []\n    for tweet in l:\n        list_of_lengths.append(len(tweet))\n        if(len(tweet) > max_size):\n            max_size = len(tweet)\n    array_of_max = np.zeros(100, dtype=np.float64)\n    \n    padded_tweets = [tweet + [array_of_max] * (max_size - len(tweet)) for tweet in l]\n    t = torch.tensor(padded_tweets, device = device)\n    t1 = torch.tensor(list_of_lengths)\n    return t,t1\n\nword_embeddings = [get_word_embeddings(tokenized_tweet, w2v_model) for tokenized_tweet in df_read['Tokenized_text']]\nword_embeddings_valid = [get_word_embeddings(tokenized_tweet, w2v_model) for tokenized_tweet in valid_df_read['Tokenized_text']]\nword_embeddings_test = [get_word_embeddings(tokenized_tweet, w2v_model) for tokenized_tweet in test_df_read['Tokenized_text']]\n\nX_train,X_train_length = padding_and_torching(word_embeddings)\nX_valid,X_valid_length = padding_and_torching(word_embeddings_valid)\nX_test,X_test_lenth = padding_and_torching(word_embeddings_test)\n\n\n# Convert to tensors\n# X_train = torch.tensor([get_embeddings_by_mean(tweet) for tweet in word_embeddings])\n# X_valid = torch.tensor([get_embeddings_by_mean(tweet) for tweet in word_embeddings_valid])\n# X_test = torch.tensor([get_embeddings_by_mean(tweet) for tweet in word_embeddings_test])\n\n# # Convert labels to tensors\nY_train = torch.tensor(df_read['Label_Map'],device = device)\nY_valid = torch.tensor(valid_df_read['Label_Map'], device = device)\n\n\nX_train = X_train.to(device)\nX_valid = X_valid.to(device)\nX_test = X_test.to(device)\n\nY_train = Y_train.to(device)\nY_valid = Y_valid.to(device)\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(X_valid.shape)\nprint(Y_valid.shape)\nprint(X_test.shape)\nprint(X_train_length.shape)\nprint(X_valid_length.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Roc curve","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\n\ndef plot_roc_curve(y_true, probabilities):\n    # Assuming probabilities is a list of numpy arrays\n#     probabilities = np.vstack(probabilities)\n    probabilities = np.vstack([p.detach().cpu().numpy() for p in probabilities])\n\n    # Binarize the labels\n    y_true_bin = label_binarize(y_true, classes=[0, 1, 2])\n\n    num_classes = probabilities.shape[1]\n\n    plt.figure(figsize=(8, 8))  # Adjust the figure size as needed\n\n    for i in range(num_classes):\n        fpr, tpr, _ = roc_curve(y_true_bin[:, i], probabilities[:, i])\n        roc_auc = auc(fpr, tpr)\n\n        plt.plot(fpr, tpr, lw=2, label=f'Class {i} (AUC = {roc_auc:.2f})')\n\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Rnn class","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass RNN(nn.Module):\n    \"\"\"\n      Wrapper module for RNN, LSTM, GRU cells using pytorch api. The output is fed to a FFN for classification.\n    \"\"\"\n    def __init__(self, cell_type,input_size, hidden_size, num_layers, drop, device, output_size, attention,num_of_heads, skip=False):\n        \n        super(RNN, self).__init__()\n        cells = {\n          \"LSTM\"    : nn.LSTM,\n          \"GRU\"     : nn.GRU\n        }\n        \n        self.cell_type = cell_type\n        self.hidden_size = hidden_size\n        self.attention = attention\n        self.num_layers=num_layers\n        self.skip= skip\n        self.dropout = nn.Dropout(drop)\n        \n        if skip:\n            self.rnn = nn.ModuleList()\n            self.rnn.append(cells[cell_type](input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True, bidirectional=True, device=device))\n            for i in range(num_layers-1):\n                self.rnn.append(cells[cell_type](input_size=hidden_size*2, hidden_size=hidden_size, num_layers=1, batch_first=True,  bidirectional=True, device=device))\n        else:\n            self.rnn = cells[cell_type](input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, dropout=drop, bidirectional=True, device=device)\n        \n        if self.attention:\n            self.Q = nn.Linear(hidden_size*2,hidden_size*2, bias=False, device=device)\n            self.K = nn.Linear(hidden_size*2,hidden_size*2, bias=False, device=device)\n            self.V = nn.Linear(hidden_size*2,hidden_size*2, bias=False, device=device)\n            self.att = nn.MultiheadAttention(self.hidden_size*2, num_heads=num_of_heads ,batch_first=True, device=device)\n\n        self.out = nn.Linear(2*hidden_size, output_size, device=device)\n\n    def forward(self, x, lengths):\n#         print(x.data.shape)\n#         x,_ = unpackedOutput= torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n#         print(x.shape)\n        if self.skip:\n            x_packed = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n            x_packed = x_packed.float().to(device)\n            if self.cell_type == 'LSTM':\n                r_out_packed, (h_n, h_c) = self.rnn[0](x_packed) \n                \n            else: \n                r_out_packed, h_n = self.rnn[0](x_packed)\n            r_out,_ = torch.nn.utils.rnn.pad_packed_sequence(r_out_packed, batch_first=True)\n            for i in range(self.num_layers-1):\n                prev = r_out\n                r_out = self.dropout(r_out)\n                x_packed = torch.nn.utils.rnn.pack_padded_sequence(r_out, lengths, batch_first=True, enforce_sorted=False)\n                x_packed = x_packed.float().to(device)\n                if self.cell_type == 'LSTM':\n                    r_out_packed, (h_n, h_c) = self.rnn[i+1](x_packed) \n                else:\n                    r_out_packed, h_n = self.rnn[i+1](x_packed)\n                r_out,_ = torch.nn.utils.rnn.pad_packed_sequence(r_out_packed, batch_first=True)\n                r_out = r_out + prev\n        else:\n            x_packed = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n            x_packed = x_packed.float().to(device)\n            if self.cell_type == 'LSTM':\n                r_out_packed, (h_n, h_c) = self.rnn(x_packed) \n            else:\n                r_out_packed, h_n = self.rnn(x_packed)\n            r_out ,_ = torch.nn.utils.rnn.pad_packed_sequence(r_out_packed, batch_first=True)\n        if self.attention:\n            Q = self.Q(r_out)\n            K = self.K(r_out)\n            V = self.V(r_out)\n            att, _ = self.att(Q, K, V)\n            out = att + r_out \n        else:\n            out = r_out\n        out = self.out(torch.cat((out[:, -1, :hidden_size], out[:, 0, hidden_size:]), dim=1))\n        probabilities = F.softmax(out, dim=1)\n        return probabilities","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset class","metadata":{}},{"cell_type":"code","source":"# Define the dataset class\nclass TextDataset:\n    def __init__(self, features, targets, lengths):\n        self.features = features\n        self.targets = targets\n        self.lengths = lengths\n        \n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, item):\n        return {\n            \"x\": self.features[item],\n            \"y\": self.targets[item],\n            \"l\": self.lengths[item]\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initialize","metadata":{}},{"cell_type":"code","source":"from torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import Dataset, DataLoader\n\nhidden_size = 30\noutput_size = 3\nnum_layers = 2\ncell_type = 'LSTM'\nnum_of_heads = 5\nnet = RNN(cell_type,  X_train.shape[2], hidden_size,num_layers,0.3, device, 3,False,num_of_heads, False)\nnet.to('cuda') \nEPOCH = 13\nLR = 0.0012\n\noptimizer = torch.optim.Adam(net.parameters(), lr=LR)   # optimize all net parameters\nloss_func = nn.CrossEntropyLoss()\n\ndataset = TextDataset(X_train, Y_train,X_train_length)\ntrain_loader = DataLoader(dataset, batch_size=128, shuffle=True)\n\nvalidation_set = TextDataset(X_valid, Y_valid, X_valid_length)\nvalid_loader = DataLoader(validation_set, batch_size=128, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# Start train mode\nnet.train()\n\n# Accumulate losses\ntrain_loss = []\nvalid_loss = []\n# Count the number of batches given to the model\nstep_count = 0\n\n\n# Iterate over the whole dataset for one epoch\nfor epoch in range(EPOCH):\n    temp_train_loss = []\n    temp_valid_loss = []\n    # Generate batch data\n    for step,data in enumerate(train_loader):\n        b_x = data['x']\n        b_y = data['y']\n        b_l = data['l']\n        \n        # clear gradients for this training step\n        optimizer.zero_grad()\n\n        output = net(b_x,b_l)\n        loss = loss_func(output, b_y)\n        \n        # For visualization        \n        temp_train_loss.append(loss.item())\n        step_count += 1\n\n        # backpropagation - compute gradients\n        loss.backward()\n        nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n        # apply gradients\n        optimizer.step()\n    total_correct = 0\n    total_samples = 0    \n#         Do not calculate gradients\n    with torch.no_grad():\n        # Faster inference\n        net.eval()\n        for step, data in enumerate(valid_loader):\n            b__x = data['x']\n            b__y = data['y']\n            b__l = data['l']\n            \n#             packed_inputs_valid = torch.nn.utils.rnn.pack_padded_sequence(b__x, b__l, batch_first=True, enforce_sorted=False)\n#             packed_inputs_valid = packed_inputs_valid.float()\n#             packed_inputs_valid = packed_inputs_valid.to('cuda')\n            test_output = net(b__x,b__l)\n            loss = loss_func(test_output, b__y)\n            \n            # For visualization        \n            temp_valid_loss.append(loss.item())\n\n            # Get predicted class\n            pred_y = torch.argmax(test_output, dim=1).squeeze()\n            \n            # Calculate accuracy\n            total_correct += (pred_y == b__y).sum().item()\n            total_samples += b__y.size(0)\n            \n        net.train()\n        \n    accuracy = total_correct / total_samples\n    train_l = mean(temp_train_loss)\n    valid_l = mean(temp_valid_loss)\n    train_loss.append(train_l)\n    valid_loss.append(valid_l)\n    # Return to training mode\n    print('Epoch: ', epoch, '| train loss: %.4f' % train_l,'valid loss: %.4f' %valid_l , '| test accuracy: %.2f' % accuracy)\n\n\n# Plot results\nplt.figure(figsize=(12, 5))\nplt.plot(train_loss, label='Train Loss')\nplt.plot(valid_loss, label='Validation Loss')\nplt.title('Learning Curve')\nplt.xlabel('Epochs', fontsize=12)\nplt.ylabel('Cross Entropy Loss', fontsize=12)\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Make: {len(X_valid)} predictions')\nprint(len(X_valid))\ntest_output = net(X_valid,X_valid_length)\n\ny_pred = torch.argmax(test_output.cpu(), dim=1).data.numpy().squeeze()\n\ny_true = Y_valid.cpu().data.numpy().squeeze()\n\nprint(y_true, 'real number')\nprint(y_pred, 'prediction number')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n\n# Assuming you already have y_true and y_pred defined\n# classification_report_str is a string returned by classification_report\nclassification_report_str = classification_report(y_true, y_pred)\n\n# Parsing the classification report string\nclassification_report_lines = classification_report_str.split('\\n')\n\n# Extracting precision, recall, and F1 score from the second last line (class average line) of classification report\nmetrics = classification_report_lines[-2].split()\n# Index 1 corresponds to precision, index 2 to recall, index 3 to f1-score\naccuracy = accuracy_score(y_true, y_pred)\nprecision = metrics[2]\nrecall = metrics[3]\nf1_score = metrics[4]\n\n# Printing the precision, recall, and F1 score\nprint(f'Accuracy: {accuracy}')\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')\nprint(f'F1 Score: {f1_score}')\n\n# Printing the confusion matrix\nprint('Confusion Matrix:')\nprint(confusion_matrix(y_true, y_pred))\nprint(len(y_true))\nplot_roc_curve(y_true, test_output)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create output file","metadata":{}},{"cell_type":"code","source":"import csv\n\nfinal_output = net(X_test,X_test_lenth)\npredictions = torch.argmax(final_output.cpu(), dim=1).data.numpy().squeeze()\nnumber_to_sentiment = {\n    0: \"NEGATIVE\",\n    1: \"NEUTRAL\",\n    2: \"POSITIVE\",\n}\n\narray = [number_to_sentiment[value] for value in predictions]\n\nwith open(\"submission.csv\", mode='w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow([\"Id\", \"Predicted\"])\n\n    for idx, word in enumerate(array, start=1):\n        writer.writerow([idx, word])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optuna optimizer","metadata":{}},{"cell_type":"code","source":"%pip install optuna","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\n\n# 1. Define an objective function to be maximized.\ndef objective(trial):\n\n    num_layers = trial.suggest_int('num_layers', 1, 3)\n    hidden_size =  trial.suggest_categorical('hidden_size', [2,10,30,64])\n    output_size = 3\n    cell_type = trial.suggest_categorical('cell_type', ['LSTM', 'GRU'])\n    attention = trial.suggest_categorical('attention', [True, False])\n    skip_connections = trial.suggest_categorical('skip_connections', [True, False])\n    dropout = trial.suggest_categorical('dropout', [0.0,0.1,0.2,0.3,0.4])\n    LR = trial.suggest_float('LR', 0.0001,0.1)\n    gc = trial.suggest_categorical('gc', [True, False])\n    num_of_heads = trial.suggest_categorical('num_of_heads' ,[1,2,3,4,5])\n    net = RNN(cell_type,  X_train.shape[2], hidden_size,num_layers,dropout, device, output_size,attention,2,skip_connections)\n    net.to('cuda') \n    print(net)\n    EPOCH = trial.suggest_int('EPOCH',10,20)\n    optimizer = torch.optim.Adam(net.parameters(), lr=LR)   # optimize all net parameters\n    loss_func = nn.CrossEntropyLoss()\n    \n    net.train()\n\n    # Accumulate losses\n    train_loss = []\n    valid_loss = []\n    # Count the number of batches given to the model\n    step_count = 0\n    # Iterate over the whole dataset for one epoch\n    for epoch in range(EPOCH):\n        temp_train_loss = []\n        temp_valid_loss = []\n        # Generate batch data\n        for step,data in enumerate(train_loader):\n            b_x = data['x']\n            b_y = data['y']\n            b_l = data['l']\n\n            # clear gradients for this training step\n            optimizer.zero_grad()\n\n            output = net(b_x,b_l)\n            loss = loss_func(output, b_y)\n\n            # For visualization        \n            temp_train_loss.append(loss.item())\n            step_count += 1\n\n            # backpropagation - compute gradients\n            loss.backward()\n            if gc:\n                nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n            # apply gradients\n            optimizer.step()\n        total_correct = 0\n        total_samples = 0    \n    #         Do not calculate gradients\n        with torch.no_grad():\n            # Faster inference\n            net.eval()\n            for step, data in enumerate(valid_loader):\n                b__x = data['x']\n                b__y = data['y']\n                b__l = data['l']\n\n    #             packed_inputs_valid = torch.nn.utils.rnn.pack_padded_sequence(b__x, b__l, batch_first=True, enforce_sorted=False)\n    #             packed_inputs_valid = packed_inputs_valid.float()\n    #             packed_inputs_valid = packed_inputs_valid.to('cuda')\n                test_output = net(b__x,b__l)\n                loss = loss_func(test_output, b__y)\n\n                # For visualization        \n                temp_valid_loss.append(loss.item())\n\n                # Get predicted class\n                pred_y = torch.argmax(test_output, dim=1).squeeze()\n\n                # Calculate accuracy\n                total_correct += (pred_y == b__y).sum().item()\n                total_samples += b__y.size(0)\n\n            net.train()\n\n        accuracy = total_correct / total_samples\n        train_l = mean(temp_train_loss)\n        valid_l = mean(temp_valid_loss)\n        train_loss.append(train_l)\n        valid_loss.append(valid_l)\n        # Return to training mode\n        print('Epoch: ', epoch, '| train loss: %.4f' % train_l,'valid loss: %.4f' %valid_l , '| test accuracy: %.4f' % accuracy)\n\n\n    # Plot results\n    plt.figure(figsize=(12, 5))\n    plt.plot(train_loss, label='Train Loss')\n    plt.plot(valid_loss, label='Validation Loss')\n    plt.title('Learning Curve')\n    plt.xlabel('Epochs', fontsize=12)\n    plt.ylabel('Cross Entropy Loss', fontsize=12)\n    plt.legend()\n    plt.show()\n    \n    print(f'Make: {len(X_valid)} predictions')\n    print(len(X_valid))\n    test_output = net(X_valid,X_valid_length)\n\n    y_pred = torch.argmax(test_output.cpu(), dim=1).data.numpy().squeeze()\n\n    y_true = Y_valid.cpu().data.numpy().squeeze()\n\n    print(y_true, 'real number')\n    print(y_pred, 'prediction number')\n    \n    classification_report_str = classification_report(y_true, y_pred)\n\n    # Parsing the classification report string\n    classification_report_lines = classification_report_str.split('\\n')\n\n    # Extracting precision, recall, and F1 score from the second last line (class average line) of classification report\n    metrics = classification_report_lines[-2].split()\n    # Index 1 corresponds to precision, index 2 to recall, index 3 to f1-score\n    accuracy = accuracy_score(y_true, y_pred)\n    precision = metrics[2]\n    recall = metrics[3]\n    f1_score = metrics[4]\n\n    # Printing the precision, recall, and F1 score\n    print(f'Accuracy: {accuracy}')\n    print(f'Precision: {precision}')\n    print(f'Recall: {recall}')\n    print(f'F1 Score: {f1_score}')\n\n    # Printing the confusion matrix\n    print('Confusion Matrix:')\n    print(confusion_matrix(y_true, y_pred))\n    print(len(y_true))\n    plot_roc_curve(y_true, test_output)\n    return float(f1_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run","metadata":{}},{"cell_type":"markdown","source":"I had make a run of 100 trials to find the optimal parameters, now for faster submission I write hyparameters hardcore and I change num of trials to 1","metadata":{}},{"cell_type":"code","source":"study = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=1)\nprint(\"Number of finished trials: \", len(study.trials))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: \", trial.value)\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from optuna.visualization import plot_contour\nfrom optuna.visualization import plot_edf\nfrom optuna.visualization import plot_intermediate_values\nfrom optuna.visualization import plot_optimization_history\nfrom optuna.visualization import plot_parallel_coordinate\nfrom optuna.visualization import plot_param_importances\nfrom optuna.visualization import plot_slice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_optimization_history(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_parallel_coordinate(study)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}