{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":64789,"databundleVersionId":7104041,"sourceType":"competition"}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch torchvision gensim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python -m spacy download el_core_news_sm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install --upgrade pandas","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom unidecode import unidecode\nimport re\nimport spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem import PorterStemmer\nspacy.load('el_core_news_sm')\nnltk.download('punkt')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nnlp = spacy.load(\"el_core_news_sm\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Data","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\ndf = pd.read_csv(\"/kaggle/input/ys19-2023-assignment-2/train_set.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/ys19-2023-assignment-2/test_set.csv\")\nvalid_df = pd.read_csv(\"/kaggle/input/ys19-2023-assignment-2/valid_set.csv\")\n\ndf['Label_Map'] = df['Sentiment'].map({\n    'NEGATIVE' : 0, 'NEUTRAL' : 1, 'POSITIVE' : 2\n})\nvalid_df['Label_Map'] = valid_df['Sentiment'].map({\n    'NEGATIVE' : 0, 'NEUTRAL' : 1, 'POSITIVE' : 2\n})","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing functions","metadata":{}},{"cell_type":"code","source":"def lowercase_without_accentds(text):\n    chars = \"ΆΈΊΌΎΉάέίόύήώ\"\n    lchars = \"αειουηαειουηω\"\n    return text.translate(str.maketrans(chars,lchars))\n\ndef extract_greek_characters(input_string):\n    greek_pattern = re.compile('[Α-Ωα-ω]+')\n    greek_characters = greek_pattern.findall(input_string)\n    result = ' '.join(greek_characters)\n    return result\n\ngreek_stopwords = [\"ο\",\"ειναι\",\"απο\",\"https\",\"co\",\"λεπω\",\"λεμε\",\"ειπα\",\"ειπες\",\"εβαλα\",\"αδιακοπα\",\"αι\",\"ακομα\",\"ακομη\",\"ακριβως\",\"αληθεια\",\"αληθινα\",\"αλλα\",\"αλλαχου\",\"αλλες\",\"αλλη\",\"αλλην\",\"αλλης\",\"αλλιως\",\"αλλιωτικα\",\"αλλο\",\"αλλοι\",\"αλλοιως\",\"αλλοιωτικα\",\"αλλον\",\"αλλος\",\"αλλοτε\",\"αλλου\",\"αλλους\",\"αλλων\",\"αμα\",\"αμεσα\",\"αμεσως\",\"αν\",\"ανα\",\"αναμεσα\",\"αναμεταξυ\",\"ανευ\",\"αντι\",\"αντιπερα\",\"αντις\",\"ανω\",\"ανωτερω\",\"αξαφνα\",\"απ\",\"απεναντι\",\"απο\",\"αποψε\",\"αρα\",\"αραγε\",\"αργα\",\"αργοτερο\",\"αριστερα\",\"αρκετα\",\"αρχικα\",\"ας\",\"αυριο\",\"αυτα\",\"αυτες\",\"αυτη\",\"αυτην\",\"αυτης\",\"αυτο\",\"αυτοι\",\"αυτον\",\"αυτος\",\"αυτου\",\"αυτους\",\"αυτων\",\"αφοτου\",\"αφου\",\"βεβαια\",\"βεβαιοτατα\",\"γι\",\"για\",\"γρηγορα\",\"γυρω\",\"δα\",\"δε\",\"δεινα\",\"δεν\",\"δεξια\",\"δηθεν\",\"δηλαδη\",\"δι\",\"δια\",\"διαρκως\",\"δικα\",\"δικο\",\"δικοι\",\"δικος\",\"δικου\",\"δικους\",\"διολου\",\"διπλα\",\"διχως\",\"εαν\",\"εαυτο\",\"εαυτον\",\"εαυτου\",\"εαυτους\",\"εαυτων\",\"εγκαιρα\",\"εγκαιρως\",\"εγω\",\"εδω\",\"ειδεμη\",\"ειθε\",\"ειμαι\",\"ειμαστε\",\"ειναι\",\"εις\",\"εισαι\",\"εισαστε\",\"ειστε\",\"ειτε\",\"ειχα\",\"ειχαμε\",\"ειχαν\",\"ειχατε\",\"ειχε\",\"ειχες\",\"εκαστα\",\"εκαστες\",\"εκαστη\",\"εκαστην\",\"εκαστης\",\"εκαστο\",\"εκαστοι\",\"εκαστον\",\"εκαστος\",\"εκαστου\",\"εκαστους\",\"εκαστων\",\"εκει\",\"εκεινα\",\"εκεινες\",\"εκεινη\",\"εκεινην\",\"εκεινης\",\"εκεινο\",\"εκεινοι\",\"εκεινον\",\"εκεινος\",\"εκεινου\",\"εκεινους\",\"εκεινων\",\"εκτος\",\"εμας\",\"εμεις\",\"εμενα\",\"εμπρος\",\"εν\",\"ενα\",\"εναν\",\"ενας\",\"ενος\",\"εντελως\",\"εντος\",\"εντωμεταξυ\",\"ενω\",\"εξ\",\"εξαφνα\",\"εξης\",\"εξισου\",\"εξω\",\"επανω\",\"επειδη\",\"επειτα\",\"επι\",\"επισης\",\"επομενως\",\"εσας\",\"εσεις\",\"εσενα\",\"εστω\",\"εσυ\",\"ετερα\",\"ετεραι\",\"ετερας\",\"ετερες\",\"ετερη\",\"ετερης\",\"ετερο\",\"ετεροι\",\"ετερον\",\"ετερος\",\"ετερου\",\"ετερους\",\"ετερων\",\"ετουτα\",\"ετουτες\",\"ετουτη\",\"ετουτην\",\"ετουτης\",\"ετουτο\",\"ετουτοι\",\"ετουτον\",\"ετουτος\",\"ετουτου\",\"ετουτους\",\"ετουτων\",\"ετσι\",\"ευγε\",\"ευθυς\",\"ευτυχως\",\"εφεξης\",\"εχει\",\"εχεις\",\"εχετε\",\"εχθες\",\"εχομε\",\"εχουμε\",\"εχουν\",\"εχτες\",\"εχω\",\"εως\",\"η\",\"ηδη\",\"ημασταν\",\"ημαστε\",\"ημουν\",\"ησασταν\",\"ησαστε\",\"ησουν\",\"ηταν\",\"ητανε\",\"ητοι\",\"ηττον\",\"θα\",\"ι\",\"ιδια\",\"ιδιαν\",\"ιδιας\",\"ιδιες\",\"ιδιο\",\"ιδιοι\",\"ιδιον\",\"ιδιος\",\"ιδιου\",\"ιδιους\",\"ιδιων\",\"ιδιως\",\"ιι\",\"ιιι\",\"ισαμε\",\"ισια\",\"ισως\",\"καθε\",\"καθεμια\",\"καθεμιας\",\"καθενα\",\"καθενας\",\"καθενος\",\"καθετι\",\"καθολου\",\"καθως\",\"και\",\"κακα\",\"κακως\",\"καλα\",\"καλως\",\"καμια\",\"καμιαν\",\"καμιας\",\"καμποσα\",\"καμποσες\",\"καμποση\",\"καμποσην\",\"καμποσης\",\"καμποσο\",\"καμποσοι\",\"καμποσον\",\"καμποσος\",\"καμποσου\",\"καμποσους\",\"καμποσων\",\"κανεις\",\"κανεν\",\"κανενα\",\"κανεναν\",\"κανενας\",\"κανενος\",\"καποια\",\"καποιαν\",\"καποιας\",\"καποιες\",\"καποιο\",\"καποιοι\",\"καποιον\",\"καποιος\",\"καποιου\",\"καποιους\",\"καποιων\",\"καποτε\",\"καπου\",\"καπως\",\"κατ\",\"κατα\",\"κατι\",\"κατιτι\",\"κατοπιν\",\"κατω\",\"κιολας\",\"κλπ\",\"κοντα\",\"κτλ\",\"κυριως\",\"λιγακι\",\"λιγο\",\"λιγωτερο\",\"λογω\",\"λοιπα\",\"λοιπον\",\"μα\",\"μαζι\",\"μακαρι\",\"μακρυα\",\"μαλιστα\",\"μαλλον\",\"μας\",\"με\",\"μεθαυριο\",\"μειον\",\"μελει\",\"μελλεται\",\"μεμιας\",\"μεν\",\"μερικα\",\"μερικες\",\"μερικοι\",\"μερικους\",\"μερικων\",\"μεσα\",\"μετ\",\"μετα\",\"μεταξυ\",\"μεχρι\",\"μη\",\"μηδε\",\"μην\",\"μηπως\",\"μητε\",\"μια\",\"μιαν\",\"μιας\",\"μολις\",\"μολονοτι\",\"μοναχα\",\"μονες\",\"μονη\",\"μονην\",\"μονης\",\"μονο\",\"μονοι\",\"μονομιας\",\"μονος\",\"μονου\",\"μονους\",\"μονων\",\"μου\",\"μπορει\",\"μπορουν\",\"μπραβο\",\"μπρος\",\"να\",\"ναι\",\"νωρις\",\"ξανα\",\"ξαφνικα\",\"ο\",\"οι\",\"ολα\",\"ολες\",\"ολη\",\"ολην\",\"ολης\",\"ολο\",\"ολογυρα\",\"ολοι\",\"ολον\",\"ολονεν\",\"ολος\",\"ολοτελα\",\"ολου\",\"ολους\",\"ολων\",\"ολως\",\"ολωςδιολου\",\"ομως\",\"οποια\",\"οποιαδηποτε\",\"οποιαν\",\"οποιανδηποτε\",\"οποιας\",\"οποιαςδηποτε\",\"οποιδηποτε\",\"οποιες\",\"οποιεςδηποτε\",\"οποιο\",\"οποιοδηποτε\",\"οποιοι\",\"οποιον\",\"οποιονδηποτε\",\"οποιος\",\"οποιοςδηποτε\",\"οποιου\",\"οποιουδηποτε\",\"οποιους\",\"οποιουςδηποτε\",\"οποιων\",\"οποιωνδηποτε\",\"οποτε\",\"οποτεδηποτε\",\"οπου\",\"οπουδηποτε\",\"οπως\",\"ορισμενα\",\"ορισμενες\",\"ορισμενων\",\"ορισμενως\",\"οσα\",\"οσαδηποτε\",\"οσες\",\"οσεςδηποτε\",\"οση\",\"οσηδηποτε\",\"οσην\",\"οσηνδηποτε\",\"οσης\",\"οσηςδηποτε\",\"οσο\",\"οσοδηποτε\",\"οσοι\",\"οσοιδηποτε\",\"οσον\",\"οσονδηποτε\",\"οσος\",\"οσοςδηποτε\",\"οσου\",\"οσουδηποτε\",\"οσους\",\"οσουςδηποτε\",\"οσων\",\"οσωνδηποτε\",\"οταν\",\"οτι\",\"οτιδηποτε\",\"οτου\",\"ου\",\"ουδε\",\"ουτε\",\"οχι\",\"παλι\",\"παντοτε\",\"παντου\",\"παντως\",\"παρα\",\"περα\",\"περι\",\"περιπου\",\"περισσοτερο\",\"περσι\",\"περυσι\",\"πια\",\"πιθανον\",\"πιο\",\"πισω\",\"πλαι\",\"πλεον\",\"πλην\",\"ποια\",\"ποιαν\",\"ποιας\",\"ποιες\",\"ποιο\",\"ποιοι\",\"ποιον\",\"ποιος\",\"ποιου\",\"ποιους\",\"ποιων\",\"πολυ\",\"ποσες\",\"ποση\",\"ποσην\",\"ποσης\",\"ποσοι\",\"ποσος\",\"ποσους\",\"ποτε\",\"που\",\"πουθε\",\"πουθενα\",\"πρεπει\",\"πριν\",\"προ\",\"προκειμενου\",\"προκειται\",\"προπερσι\",\"προς\",\"προτου\",\"προχθες\",\"προχτες\",\"πρωτυτερα\",\"πως\",\"σαν\",\"σας\",\"σε\",\"σεις\",\"σημερα\",\"σιγα\",\"σου\",\"στα\",\"στη\",\"στην\",\"στης\",\"στις\",\"στο\",\"στον\",\"στου\",\"στους\",\"στων\",\"συγχρονως\",\"συν\",\"συναμα\",\"συνεπως\",\"συνηθως\",\"συχνα\",\"συχνας\",\"συχνες\",\"συχνη\",\"συχνην\",\"συχνης\",\"συχνο\",\"συχνοι\",\"συχνον\",\"συχνος\",\"συχνου\",\"συχνου\",\"συχνους\",\"συχνων\",\"συχνως\",\"σχεδον\",\"σωστα\",\"τα\",\"ταδε\",\"ταυτα\",\"ταυτες\",\"ταυτη\",\"ταυτην\",\"ταυτης\",\"ταυτο,ταυτον\",\"ταυτος\",\"ταυτου\",\"ταυτων\",\"ταχα\",\"ταχατε\",\"τελικα\",\"τελικως\",\"τες\",\"τετοια\",\"τετοιαν\",\"τετοιας\",\"τετοιες\",\"τετοιο\",\"τετοιοι\",\"τετοιον\",\"τετοιος\",\"τετοιου\",\"τετοιους\",\"τετοιων\",\"τη\",\"την\",\"της\",\"τι\",\"τιποτα\",\"τιποτε\",\"τις\",\"το\",\"τοι\",\"τον\",\"τος\",\"τοσα\",\"τοσες\",\"τοση\",\"τοσην\",\"τοσης\",\"τοσο\",\"τοσοι\",\"τοσον\",\"τοσος\",\"τοσου\",\"τοσους\",\"τοσων\",\"τοτε\",\"του\",\"τουλαχιστο\",\"τουλαχιστον\",\"τους\",\"τουτα\",\"τουτες\",\"τουτη\",\"τουτην\",\"τουτης\",\"τουτο\",\"τουτοι\",\"τουτοις\",\"τουτον\",\"τουτος\",\"τουτου\",\"τουτους\",\"τουτων\",\"τυχον\",\"των\",\"τωρα\",\"υπ\",\"υπερ\",\"υπο\",\"υποψη\",\"υποψιν\",\"υστερα\",\"φετος\",\"χαμηλα\",\"χθες\",\"χτες\",\"χωρις\",\"χωριστα\",\"ψηλα\",\"ω\",\"ωραια\",\"ως\",\"ωσαν\",\"ωσοτου\",\"ωσπου\",\"ωστε\",\"ωστοσο\",\"ωχ\",\"ο\",\"η\",\"το\",\"τα\",\"τη\",\"δηλαδη\",\"μεχρι\",\"γιατι\",\"εχω\",\"στους\",\"μια\",\"ένας\",\"μία\",\"κάποιος\",\"κάποια\",\"κάποιο\",\"κάποιοι\",\"αυτος\",\"αυτη\",\"αυτο\",\"αυτοι\",\"αυτες\",\"αυτα\",\"στο\",\"στη\",\"στα\",\"για\",\"με\",\"απο\",\"προς\",\"ειναι\",\"εχει\",\"εχουν\",\"θα\",\"δεν\",\"πανω\",\"κατω\",\"μεσα\",\"εξω\",\"κατω\",\"ως\",\"πανω\",\"κατω\",\"πιο\",\"εδω\",\"εκει\",\"πολυ\",\"λιγο\",\"τωρα\",\"ακομα\",\"ομως\",\"επισης\",\"παντα\",\"ακομη\",\"πιθανως\",\"μονο\",\"οχι\",\"ναι\",\"ευχαριστως\",\"γενικα\",\"ολοι\",\"ολες\",\"ολα\",\"ποιος\",\"ποια\",\"ποιο\",\"ποιοι\",\"ποιες\",\"τιποτα\",\"κανεις\",\"καμια\",\"κανενα\",\"κανενες\",\"αυτος\",\"αυτη\",\"αυτο\",\"αυτοι\",\"αυτες\",\"αυτα\",\"απο\",\"σε\",\"υπο\",\"μετα\",\"πριν\",\"επειτα\",\"αντι\",\"εναν\",\"μιαν\",\"κανεναν\",\"καμιαν\",\"κανενα\",\"καμια\",\"μιαν\",\"ενα\",\"οποιος\",\"οποια\",\"οποιο\",\"οποιοι\",\"οποιες\",\"οποιαν\",\"οποιον\",\"ολος\",\"ολη\",\"ολα\",\"ολους\",\"ολες\",\"ολων\",\"καθενας\",\"καθεμια\",\"καθενα\",\"καθενες\",\"ακομα\",\"ενω\",\"επομενως\",\"συνεπως\",\"επιπλεον\",\"παρολα αυτα\",\"παρ ολα αυτα\",\"επισης\",\"και\",\"αλλα\",\"αλλα και\",\"αν\",\"εαν\",\"αν και\",\"αντι\",\"αντι να\",\"αντι το\",\"αντι τα\",\"αντι του\",\"αντι τη\",\"αντι των\",\"αντι στο\",\"αντι στη\",\"αντι στα\",\"αντι στου\",\"αντι στην\",\"αντι στις\",\"αντι στον\",\"μεσω\",\"τους\",\"μας\",\"ηταν\",\"εκ\",\"φορα\",\"πρωτη\",\"ειχα\",\"εμεις\",\"εσεις\",\"ηδη\",\"απ\",\"εγινε\",\"ειχε\",\"αλλα\",\"ουτε\",\"ενας\",\"εσας\",\"αυτοι\",\"αυτο\",\"νεα\",\"οντως\",\"θελετε\",\"κανει\",\"σ\",\"μας\",\"πρεπε\",\"ε\",\"μαλιστα\",\"τους\",\"ηθελε\",\"παω\",\"εβαλε\",\"λεει\",\"γ\",\"ν\",\"θες\",\"ερχεται\",\"διαρκεια\",\"θελουν\",\"ασε\",\"χ\",\"λες\",\"ξερω\",\"α\",\"δω\",\"ειδε\",\"μπηκε\",\"βαλει\",\"μερες\",\"εφοσον\",\"ενα\",\"δυο\",\"τρια\",\"γινει\",\"εργο\",\"μιλαω\",\"μιλησε\",\"ποσα\",\"ωρες\",\"πρωινες\",\"πρωτα\",\"θελει\",\"βαζω\",\"εβαζε\",\"εναντι\",\"μπορεις\",\"βρισκει\",\"δει\",\"μπορω\",\"γινε\",\"κανουν\"]\npattern = '#!$%^&*()_+={}\\[\\]:;\"\\'<>,.?/\\\\|`~-><]'\n\ndef process_text(text):\n    \n#     new_text = extract_greek_characters(text)\n    doc = nlp(text)\n    processed_words = []\n    # Loop through each word\n    for word in doc:\n        if word.is_punct or any(char.isdigit() for char in word.text) or word.text in greek_stopwords:\n            continue\n        else:\n            clean_word = word.lemma_\n            clean_word = lowercase_without_accentds(clean_word)\n            clean_word = clean_word.lower()\n            if clean_word not in greek_stopwords:\n                processed_words.append(clean_word)\n    # Join the processed words back into a string\n    processed_text = ' '.join(processed_words)\n    return processed_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preprocessing","metadata":{}},{"cell_type":"code","source":"import os\n# os.remove('train.json')\n# os.remove('valid.json')\ntext = df['Text']\nvalid_text = valid_df['Text']\ntest_text = test_df['Text']\nprint(\"Preproccesing training data\")\ndf['processed_text'] = text.apply(process_text)\ndf.to_json('train.json', orient='records', lines=True)\nprint(\"Preproccesing valid data\")\nvalid_df['processed_text'] = valid_text.apply(process_text)\nvalid_df.to_json('valid.json', orient='records', lines=True)\ntest_df['processed_text'] = test_text.apply(process_text)\ntest_df.to_json('test.json', orient='records', lines=True)\nprint(\"Vectorization\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get preprocessed data from file","metadata":{}},{"cell_type":"code","source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Write the DataFrame to a JSON file\ndf_read = pd.read_json('train.json', lines=True)\ntrain_preprocessed_text = df_read['processed_text']\nvalid_df_read = pd.read_json('valid.json', lines=True)\nvalid_preprocessed_text = valid_df_read['processed_text']\ntest_df_read = pd.read_json('test.json', lines=True)\ntest_preprocessed_text = test_df_read['processed_text']\n\nprint(\"TRAIN\")\nprint(len(train_preprocessed_text))\nprint(\"VALID\")\nprint(len(valid_preprocessed_text))\nprint(\"TEST\")\nprint(len(test_preprocessed_text))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data tokenization","metadata":{}},{"cell_type":"code","source":"import re\nfrom nltk.tokenize import word_tokenize\n\ndef clean_and_tokenize(text):\n\n    pattern = r'[!@#$%^&*()_+={}\\[\\]:;<>,.?/~`]+'\n    \n    cleaned_text = re.sub(pattern, '', text)\n\n    tokens = cleaned_text.split()\n    \n    return tokens","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize model","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec\nimport nltk\nnltk.download('punkt')\n\n# Preprocess each sentence\ndf_read['Tokenized_text'] = train_preprocessed_text.apply(clean_and_tokenize)\nvalid_df_read['Tokenized_text'] = valid_preprocessed_text.apply(clean_and_tokenize)\ntest_df_read['Tokenized_text'] = test_preprocessed_text.apply(clean_and_tokenize)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize model","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nfrom gensim.models import Word2Vec\nimport pandas as pd\nimport numpy as np\n\n# Assuming df_read and valid_df_read are your dataframes with 'Tokenized_text' column\n\n# Combine the tokenized text from both dataframes\nall_tokenized_text = pd.concat([df_read['Tokenized_text'], valid_df_read['Tokenized_text'],test_df_read['Tokenized_text']])\n\n# Define the vector_size (you need to set it to the appropriate value)\nvector_size = 100\n\n# Visualized words\nvisualized_words = ['μητσοτακης', 'τσιπρας', 'νδ', 'συριζα', 'κουλη', 'κκε', 'πολιτης', 'τσιπρα', 'βουλευτης', 'κυριακος',\n                   'προθυπουργος' , 'πασοκ' , 'πολιτικος' , 'δημοκρατια' , 'νεος' , 'βελοπουλος']\n\ndef visualize_embeddings(embeddings, words, axs, title):\n    tsne = TSNE(init = \"pca\",n_components=2,random_state=42, perplexity=len(words) - 1)\n    embedding_vectors = np.array([embeddings[word] for word in words])\n    two_d_embeddings = tsne.fit_transform(embedding_vectors)\n\n    for i, word in enumerate(words):\n        x, y = two_d_embeddings[i, :]\n        axs.scatter(x, y)\n        axs.annotate(word, (x, y), xytext=(5, 2), textcoords=\"offset points\", ha=\"right\", va=\"bottom\")\n\n    axs.set_title(title)\n\n# Create subplots\nfig, axs = plt.subplots(2, 3, figsize=(15, 10))\n\n# Iterate over the subplots\ncount = 2\nfor i in range(2):\n    for j in range(3):\n        axs_ = axs[i, j]\n\n        # Train Word2Vec model with CBOW algorithm (sg=0)\n        w2v_model = Word2Vec(sentences=all_tokenized_text, vector_size=vector_size, window = count, min_count=1, sg=0)\n        \n        # Visualize embeddings\n        visualize_embeddings(w2v_model.wv, visualized_words, axs_, f'CBOW, Window Size: {count}')\n        count += 3\n# Show the plots\nplt.show()\n\n# Create new subplots for the second set of visualizations\nfig, axs = plt.subplots(2, 3, figsize=(15, 10))\ncount = 2\nfor i in range(2):\n    for j in range(3):\n        axs_ = axs[i, j]\n\n        # Train Word2Vec model with Skip-gram algorithm (sg=1)\n        w2v_model = Word2Vec(sentences=all_tokenized_text, vector_size=vector_size, window=count, min_count=1, sg=1)\n\n        # Visualize embeddings\n        visualize_embeddings(w2v_model.wv, visualized_words, axs_, f'Skip-gram, Window Size: {count}')\n        count += 3\n\n# Show the plots\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train word2vec\nI choose cbow algorithmn and window size = 8","metadata":{}},{"cell_type":"code","source":"vector_size = 100\nw2v_model = Word2Vec(sentences=pd.concat([df_read['Tokenized_text'], valid_df_read['Tokenized_text'],test_df_read['Tokenized_text']]), vector_size=vector_size, window=8, min_count=1, sg=0)\nw2v_model.save(\"greek_word2vec.model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create training torches","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.nn.utils.rnn import pad_sequence\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom statistics import mean\n\n# Load the Word2Vec model\nw2v_model = Word2Vec.load(\"greek_word2vec.model\")\n\n# Function to get word embeddings for a tokenized tweet\ndef get_word_embeddings(tokenized_tweet, model):\n    embeddings = []\n    if not tokenized_tweet:\n        tokenized_tweet = [\"Ολυμπιακός\"] # word without sence\n    for word in tokenized_tweet:        \n        if word in model.wv:\n            embeddings.append(model.wv[word])\n        else:\n            embeddings.append(np.zeros(model.vector_size, dtype=np.float32))\n    return embeddings\n\n# Function to get mean embeddings for a list of tweets\ndef get_embeddings_by_mean(tweets):\n    return np.mean(np.stack(tweets, axis=0), axis=0)\n\n\nword_embeddings = [get_word_embeddings(tokenized_tweet, w2v_model) for tokenized_tweet in df_read['Tokenized_text']]\nword_embeddings_valid = [get_word_embeddings(tokenized_tweet, w2v_model) for tokenized_tweet in valid_df_read['Tokenized_text']]\nword_embeddings_test = [get_word_embeddings(tokenized_tweet, w2v_model) for tokenized_tweet in test_df_read['Tokenized_text']]\n\n# Convert to tensors\nX_train = torch.tensor([get_embeddings_by_mean(tweet) for tweet in word_embeddings])\nX_valid = torch.tensor([get_embeddings_by_mean(tweet) for tweet in word_embeddings_valid])\nX_test = torch.tensor([get_embeddings_by_mean(tweet) for tweet in word_embeddings_test])\n\n# Convert labels to tensors\nY_train = torch.tensor(df['Label_Map'])\nY_valid = torch.tensor(valid_df['Label_Map'])\n\n\nprint(X_train.shape)\nprint(Y_train.shape)\nprint(X_valid.shape)\nprint(Y_valid.shape)\nprint(X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset class","metadata":{}},{"cell_type":"code","source":"# Define the dataset class\nclass TextDataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n\n    def __len__(self):\n        return self.features.shape[0]\n\n    def __getitem__(self, item):\n        return {\n            \"x\": self.features[item].clone().detach().to(torch.float),\n            \"y\": self.targets[item].clone().detach().to(torch.long)\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Engine class","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn \nfrom sklearn.metrics import f1_score\n\nimport torch.nn.functional as F\n\nclass Engine:\n    def __init__(self, model, optimizer):\n        self.model = model\n        self.optimizer = optimizer\n        \n    \n    @staticmethod\n    def loss_fn(targets, outputs):\n        return nn.CrossEntropyLoss()(outputs, targets)\n    \n    \n    def train(self,data_loader):\n        self.model.train()\n        final_loss = 0\n        for data in data_loader:\n            self.optimizer.zero_grad()\n            inputs = data[\"x\"]\n            targets = data[\"y\"]\n            outputs = self.model(inputs)\n            loss = self.loss_fn(targets,outputs)\n            loss.backward()\n            self.optimizer.step()\n            final_loss += loss.item()\n        return final_loss / len(data_loader)\n    \n    def evaluate(self,data_loader):\n        self.model.eval()\n        final_loss = 0\n        all_targets = []\n        all_outputs = []\n        all_probabilities = []\n        for data in data_loader:\n            inputs = data[\"x\"]\n            targets = data[\"y\"]\n            outputs = self.model(inputs)\n            \n            # Apply softmax to obtain probabilities\n            probabilities = F.softmax(outputs, dim=1)\n            all_probabilities.extend(probabilities.cpu().detach().numpy())\n            \n            loss = self.loss_fn(targets,outputs)\n            final_loss += loss.item()\n            \n            _, predictions = torch.max(outputs, 1)\n            all_targets.extend(targets.cpu().numpy())\n            all_outputs.extend(predictions.cpu().numpy())\n        f1_per_class = f1_score(all_targets, all_outputs, average=None)\n        f1 = f1_score(all_targets, all_outputs, average='micro')\n        return final_loss / len(data_loader), [f1,f1_per_class],all_probabilities,all_outputs\n    \n    def make_predictions(self,model, data_loader):\n        self.model.eval()\n        all_probabilities = []\n        all_outputs = []\n\n        with torch.no_grad():\n            for data in data_loader:\n                inputs = data[\"x\"]\n                outputs = self.model(inputs)\n\n                # Apply softmax to obtain probabilities\n                probabilities = F.softmax(outputs, dim=1)\n                all_probabilities.extend(probabilities.cpu().detach().numpy())\n\n                _, predictions = torch.max(outputs, 1)\n                all_outputs.extend(predictions.cpu().numpy())\n\n        return np.array(all_probabilities), np.array(all_outputs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build model","metadata":{}},{"cell_type":"code","source":"class Model(nn.Module):\n    \n    def __init__(self, nfeatures, ntargets , nlayers, hidden_size, dropout):\n        super().__init__()\n        layers = []\n        for i in range(nlayers):\n            if len(layers) == 0:\n                layers.append(nn.Linear(nfeatures,hidden_size[i]))\n#                 layers.append(nn.BatchNorm1d(hidden_size))\n#                 layers.append(nn.Dropout(dropout))\n                layers.append(nn.ReLU())\n            else:\n                layers.append(nn.Linear(hidden_size[i-1],hidden_size[i]))\n#                 layers.append(nn.BatchNorm1d(hidden_size))\n#                 layers.append(nn.Dropout(dropout))\n                layers.append(nn.ReLU())\n        layers.append(nn.Linear(hidden_size[i],ntargets))\n        self.model = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        return self.model(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Roc curve function","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\n\ndef plot_roc_curve(y_true, probabilities):\n    # Assuming probabilities is a list of numpy arrays\n    probabilities = np.vstack(probabilities)\n\n    # Binarize the labels\n    y_true_bin = label_binarize(y_true, classes=[0, 1, 2])\n\n    num_classes = probabilities.shape[1]\n\n    plt.figure(figsize=(8, 8))  # Adjust the figure size as needed\n\n    for i in range(num_classes):\n        fpr, tpr, _ = roc_curve(y_true_bin[:, i], probabilities[:, i])\n        roc_auc = auc(fpr, tpr)\n\n        plt.plot(fpr, tpr, lw=2, label=f'Class {i} (AUC = {roc_auc:.2f})')\n\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Learning curve function","metadata":{}},{"cell_type":"code","source":"def plot_learning_curve(train_losses, valid_losses):\n    plt.figure(figsize=(10, 6))\n    epochs = range(1, len(train_losses) + 1)\n\n    plt.plot(epochs, train_losses, label='Training Loss', marker='o')\n    plt.plot(epochs, valid_losses, label='Validation Loss', marker='o')\n\n    plt.title('Learning Curve')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comfusion matrix","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\ndef plot_confusion_matrix(y_true, y_pred, classes):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n    plt.title('Confusion Matrix')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"from torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import Dataset, DataLoader\n\ndef run_training(f_,params,flag,model_save_path):\n    \n    dataset = TextDataset(X_train, Y_train)\n    train_loader = DataLoader(dataset, batch_size=params[\"batch_size1\"], shuffle=True)\n\n    validation_set = TextDataset(X_valid, Y_valid)\n    valid_loader = DataLoader(validation_set, batch_size=params[\"batch_size2\"], shuffle=False)\n    \n    model = Model(nfeatures = X_train.shape[1],ntargets = 3,\n            nlayers = params[\"num_layers\"], hidden_size = params[\"hidden_size\"], dropout = params[\"dropout\"])\n    optimizer = torch.optim.Adam(model.parameters(),lr=params[\"learning_rate\"])\n    eng = Engine(model,optimizer)\n    \n    EPOCHS = 20\n    best_loss = np.inf\n    early_stopping_iter = 100\n    early_stopping_counter = 0\n    total_probabilities = []\n    train_losses = []\n    valid_losses = []\n    f1_best = 0\n    best_epoch = 0\n    r_classes_f1 = [0,0,0]\n    scheduler = StepLR(optimizer, step_size=1, gamma=0.01)\n    for epoch in range(EPOCHS):\n        train_loss = eng.train(train_loader)\n        valid_loss,f_list,probabilities,all_outputs = eng.evaluate(valid_loader)\n        f1 = f_list[0]\n        f_per_class = f_list[1]\n        train_losses.append(train_loss)\n        valid_losses.append(valid_loss)\n\n        if f1 > f1_best:\n            f1_best = f1\n            best_epoch = epoch\n            r_classes_f1 = f_per_class\n            torch.save(model.state_dict(), model_save_path)\n        else:\n            early_stopping_counter +=1\n        if early_stopping_counter > early_stopping_iter:\n            break\n    if flag == 1:\n        #plot roc curve\n        plot_roc_curve(Y_valid, probabilities)\n        plot_learning_curve(train_losses,valid_losses)\n        plot_confusion_matrix(Y_valid, all_outputs, [\"Negative, Neutral, Positive\"])\n    return best_loss,[f1_best,best_epoch,r_classes_f1],eng","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Optuna optimazer","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    params = {\n        \"num_layers\" : 3,\n        \"hidden_size\" : \n        { 0 : trial.suggest_int(\"h1\",1,300),\n          1 : trial.suggest_int(\"h2\",1,300),\n          2 : trial.suggest_int(\"h3\",1,300)\n        },\n        \n        \"dropout\" : trial.suggest_uniform(\"dropout\",0.1,0.4),\n        \"learning_rate\" : trial.suggest_uniform(\"learning_rate\", 1e-6,1e-3),\n        \"batch_size1\" : trial.suggest_int(\"batch_size1\",30,200),\n        \"batch_size2\" : trial.suggest_int(\"batch_size2\",30,200)\n    }\n    \n    print(params)\n    \n    temp_loss,details,eng_opt = run_training(0, params,0,\"optuna_best_model.pth\")\n    \n    print(f\"best_f1: {details[0]}, best_epoch {details[1]}\")\n    for i, f1 in enumerate(details[2]):\n        print(f\"Class {i}: {f1}\")\n    return details[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run optuna","metadata":{}},{"cell_type":"code","source":"import optuna\n\nstudy = optuna.create_study(direction = \"maximize\")\nstudy.optimize(objective, n_trials=1) # change number of trials\n\nprint(\"best trial:\")\ntrial_ = study.best_trial\n\nprint(trial_.values)\nprint(trial_.params)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using optimal optuna parameters","metadata":{}},{"cell_type":"code","source":"trial_.params[\"num_layers\"] = 3\ntrial_.params[\"hidden_size\"] = { \n        0 : trial_.params[\"h1\"],\n        1 : trial_.params[\"h2\"],\n        2 : trial_.params[\"h3\"]\n}\n    \nprint(trial_.params)\n\nscr,results,eng_opt = run_training(j,trial_.params,1,\"best_model.pth\")\nprint(results[0],results[1])\n\nfor i, f1 in enumerate(results[2]):\n        print(f\"Class {i}: {f1}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Using for me optimal parameters","metadata":{}},{"cell_type":"code","source":"optim_params = {\n        \"num_layers\" : 3,\n        \"hidden_size\" : \n        { 0 : 174,\n          1 : 34,\n          2 : 153\n        },\n        \n        \"dropout\" : 0.13,\n        \"learning_rate\" : 0.0001755,\n        \"batch_size1\" : 151,\n        \"batch_size2\" : 89\n    }\nsrc, results,eng = run_training(j,optim_params,1,\"best_model.pth\")\nprint(results[0],results[1])\n\nfor i, f1 in enumerate(results[2]):\n        print(f\"Class {i}: {f1}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create output file","metadata":{}},{"cell_type":"code","source":"model = torch.load('best_model.pth')\n\ndataset_test = TextDataset(X_test, Y_train)\ntest_loader = DataLoader(dataset_test, batch_size=1, shuffle=False)\n\noutput = eng.make_predictions(model,test_loader)\n\npredictions = output[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import csv\n\nnumber_to_sentiment = {\n    0: \"NEGATIVE\",\n    1: \"NEUTRAL\",\n    2: \"POSITIVE\",\n}\n\narray = [number_to_sentiment[value] for value in predictions]\n\nwith open(\"submission.csv\", mode='w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow([\"Id\", \"Predicted\"])\n\n    for idx, word in enumerate(array, start=1):\n        writer.writerow([idx, word])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}